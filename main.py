#!/usr/bin/env python3
"""
Crypto Indices Data Collection Script
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å LunarCrush API –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Supabase
"""

import requests
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import sys
from typing import Dict, List, Optional, Tuple
import warnings
from supabase import create_client, Client
import psycopg2
from psycopg2.extras import Json
import logging

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('crypto_indices.log')
    ]
)

logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è API –∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
def get_env_variable(name: str, required: bool = True) -> Optional[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    value = os.getenv(name)
    if required and not value:
        logger.error(f"–û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è {name} –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
        raise EnvironmentError(f"Missing required environment variable: {name}")
    return value

# API –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
API_URL = "https://lunarcrush.com/api4/public/coins/list/v1"
API_KEY = get_env_variable('LUNARCRUSH_API_KEY')

# Supabase –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
SUPABASE_URL = get_env_variable('SUPABASE_URL')
SUPABASE_KEY = get_env_variable('SUPABASE_KEY')

# PostgreSQL –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
POSTGRES_CONFIG = {
    "user": get_env_variable('POSTGRES_USER'),
    "password": get_env_variable('POSTGRES_PASSWORD'),
    "host": get_env_variable('POSTGRES_HOST'),
    "port": int(get_env_variable('POSTGRES_PORT', False) or 5432),
    "database": get_env_variable('POSTGRES_DATABASE')
}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Supabase
try:
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    logger.info("Supabase –∫–ª–∏–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
except Exception as e:
    logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Supabase –∫–ª–∏–µ–Ω—Ç–∞: {e}")
    raise

# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò–ù–î–ï–ö–°–û–í
INDICES = {
    "RWA": {
        "name": "Real World Assets",
        "description": "–¢–æ–∫–µ–Ω—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∞–∫—Ç–∏–≤–∞–º–∏",
        "tokens": {
            46356: "ONDO",   # Ondo Finance
            158332: "SYRUP", # Maple Finance
            18: "LINK",      # Chainlink
            159642: "PLUME", # PLUME
            158860: "USUAL", # USUAL
            149893: "ENA",   # Ethena
            2141: "XAUT",    # Tether Gold
            14846: "CFG",    # Centrifuge
            157749: "USTBL", # Spiko
            7035: "DAO",     # DAO Maker
            20358: "CPOOL",  # Clearpool
            17: "IOTA",      # IOTA
            70319: "SKY",    # Sky
            51211: "CTC",    # Creditcoin
            12112: "EPIC",   # Epic Chain
            279: "RSR",      # Reserve Rights
            158179: "USYC",  # Circle USYC
            160101: "CUSDO", # OpenDollar
            431: "TRAC",     # OriginTrail
            30504: "POLYX",  # Polymesh
            51116: "CHEX",   # Chintai
            7861: "WHITE",   # WhiteRock
            2868: "VSN",     # Vision
            152191: "ZBCN",  # Zebec Network
            159759: "COLLAT",# collaterize
            151919: "PRCL",  # Parcl
            160799: "HUMA"   # Huma Finance
        }
    },

    "DEFI": {
        "name": "DeFi",
        "description": "–ü—Ä–æ—Ç–æ–∫–æ–ª—ã –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤",
        "tokens": {
            28: "UNI",       # Uniswap
            1048: "AAVE",    # Aave
            7035: "MKR",     # MakerDAO
            3094: "COMP",    # Compound
            1844: "CRV",     # Curve
            1758: "SUSHI",   # SushiSwap
            7040: "YFI",     # Yearn Finance
            261: "SNX",      # Synthetix
            1834: "1INCH",   # 1inch
            33193: "RUNE",   # THORChain
            2477: "BAL",     # Balancer
            30208: "LDO",    # Lido DAO
            52713: "JOE",    # TraderJoe
            154003: "JUP"    # Jupiter
        }
    },

    "MEME": {
        "name": "Meme Coins",
        "description": "–ú–µ–º-—Ç–æ–∫–µ–Ω—ã –∏ community-driven –ø—Ä–æ–µ–∫—Ç—ã",
        "tokens": {
            12: "DOGE",      # Dogecoin
            285: "SHIB",     # Shiba Inu
            159246: "PEPE",  # Pepe
            70317: "FLOKI",  # Floki
            70056: "WIF",    # dogwifhat
            156269: "BONK",  # Bonk
            159989: "BRETT", # Brett
            160097: "MEW",   # cat in a dogs world
            157068: "POPCAT",# Popcat
            160321: "MOG",   # Mog Coin
            160636: "TURBO", # Turbo
            160833: "NEIRO"  # Neiro
        }
    }
}

# –ï–¥–∏–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
WEIGHTS = {
    'sentiment': 0.35,           # 35% –≤–µ—Å
    'interactions_24h': 0.15,    # 25% –≤–µ—Å
    'social_volume_24h': 0.15,   # 25% –≤–µ—Å
    'alt_rank': 0.35             # 35% –≤–µ—Å
}

# –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å–∏–≥–º–æ–∏–¥–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
NORMALIZATION_BASELINES = {
    'interactions_24h': 500000,    # –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è interactions
    'social_volume_24h': 10000,    # –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è social volume
    'alt_rank_divisor': 100        # –î–µ–ª–∏—Ç–µ–ª—å –¥–ª—è alt_rank
}


def normalize_sentiment(sentiment: float) -> float:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç sentiment (—É–∂–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-100) –≤ –¥–∏–∞–ø–∞–∑–æ–Ω 0-1"""
    return sentiment / 100 if sentiment else 0


def normalize_alt_rank(alt_rank: float) -> float:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç alt_rank –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—Ä–∞—Ç–Ω—É—é —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é.
    –ß–µ–º –º–µ–Ω—å—à–µ —Ä–∞–Ω–≥, —Ç–µ–º –ª—É—á—à–µ (1 = –ª—É—á—à–∏–π).
    """
    if alt_rank <= 0:
        return 0
    divisor = NORMALIZATION_BASELINES['alt_rank_divisor']
    return 1 / (1 + alt_rank / divisor)


def normalize_interactions(interactions: float) -> float:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç interactions –∏—Å–ø–æ–ª—å–∑—É—è —Å–∏–≥–º–æ–∏–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é."""
    if interactions < 0:
        return 0
    baseline = NORMALIZATION_BASELINES['interactions_24h']
    return interactions / (interactions + baseline)


def normalize_social_volume(volume: float) -> float:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç social_volume –∏—Å–ø–æ–ª—å–∑—É—è —Å–∏–≥–º–æ–∏–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é."""
    if volume < 0:
        return 0
    baseline = NORMALIZATION_BASELINES['social_volume_24h']
    return volume / (volume + baseline)


def calculate_weighted_index(metrics: Dict[str, float]) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –æ—Ç 1 –¥–æ 10"""
    weighted_sum = sum(metrics[key] * WEIGHTS[key] for key in WEIGHTS.keys())
    return 1 + (weighted_sum * 9)


def validate_normalized_metrics(metrics: Dict[str, float], index_name: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –≤—Å–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1"""
    valid = True
    for key, value in metrics.items():
        if value < 0 or value > 1:
            logger.warning(f"[{index_name}] –ú–µ—Ç—Ä–∏–∫–∞ {key} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [0,1]: {value}")
            valid = False
    return valid


def fetch_market_data() -> Optional[List[Dict]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å API LunarCrush"""
    headers = {
        'Authorization': f'Bearer {API_KEY}'
    }

    try:
        logger.info("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ API...")
        response = requests.get(API_URL, headers=headers, timeout=30)
        response.raise_for_status()

        data = response.json()

        if 'data' in data:
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(data['data'])} —Ç–æ–∫–µ–Ω–æ–≤")
            return data['data']
        else:
            logger.error("–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ API")
            return None

    except requests.exceptions.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {e}")
        return None
    except json.JSONDecodeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON: {e}")
        return None


def filter_index_tokens(market_data: List[Dict], index_tokens: Dict[int, str]) -> Dict[int, Dict]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
    filtered_data = {}
    missing_tokens = []

    for token_id, token_symbol in index_tokens.items():
        token_found = False

        for token_data in market_data:
            if token_data.get('id') == token_id:
                token_found = True

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω—É–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                metrics = {}
                required_fields = ['interactions_24h', 'social_volume_24h', 'sentiment', 'alt_rank']
                missing_fields = []

                for field in required_fields:
                    if field in token_data and token_data[field] is not None:
                        metrics[field] = float(token_data[field])
                    else:
                        missing_fields.append(field)
                        metrics[field] = 0

                # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                metrics['symbol'] = token_data.get('symbol', token_symbol)
                metrics['name'] = token_data.get('name', '')
                metrics['price'] = token_data.get('price', 0)
                metrics['market_cap'] = token_data.get('market_cap', 0)

                if missing_fields:
                    logger.warning(f"{token_symbol}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–æ–ª—è: {', '.join(missing_fields)}")

                filtered_data[token_id] = metrics
                break

        if not token_found:
            missing_tokens.append(token_symbol)

    if missing_tokens:
        logger.warning(f"–¢–æ–∫–µ–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {', '.join(missing_tokens)}")

    return filtered_data


def calculate_single_index(filtered_tokens: Dict[int, Dict], index_code: str, index_info: Dict) -> Optional[Dict]:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–µ–∫—Ç–æ—Ä–∞"""
    if not filtered_tokens:
        logger.error(f"[{index_code}] –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–µ–∫—Å–∞")
        return None

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
    all_interactions = [t['interactions_24h'] for t in filtered_tokens.values()]
    all_volumes = [t['social_volume_24h'] for t in filtered_tokens.values()]
    all_sentiments = [t['sentiment'] for t in filtered_tokens.values()]
    all_alt_ranks = [t['alt_rank'] for t in filtered_tokens.values() if t['alt_rank'] > 0]

    # –°—É–º–º–∞—Ä–Ω—ã–µ/—Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è (–¥–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)
    total_raw = {
        'interactions_24h': sum(all_interactions),
        'social_volume_24h': sum(all_volumes),
        'sentiment': np.mean([s for s in all_sentiments if s > 0]) if any(all_sentiments) else 0,
        'alt_rank': np.mean(all_alt_ranks) if all_alt_ranks else 500
    }

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
    normalized_metrics = {
        'interactions_24h': normalize_interactions(total_raw['interactions_24h']),
        'social_volume_24h': normalize_social_volume(total_raw['social_volume_24h']),
        'sentiment': normalize_sentiment(total_raw['sentiment']),
        'alt_rank': normalize_alt_rank(total_raw['alt_rank'])
    }

    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    validate_normalized_metrics(normalized_metrics, index_code)

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å
    index_value = calculate_weighted_index(normalized_metrics)

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    result = {
        'index_code': index_code,
        'index_name': index_info['name'],
        'description': index_info['description'],
        'timestamp': datetime.now().isoformat(),
        'index_value': round(index_value, 2),
        'tokens_count': len(filtered_tokens),
        'raw_metrics': total_raw,
        'normalized_metrics': {k: round(v, 4) for k, v in normalized_metrics.items()},
        'weights': WEIGHTS,
        'tokens': {}
    }

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É
    for token_id, data in filtered_tokens.items():
        result['tokens'][data['symbol']] = {
            'id': token_id,
            'name': data['name'],
            'price': round(data['price'], 2) if data['price'] else 0,
            'sentiment': data['sentiment'],
            'alt_rank': data['alt_rank'],
            'interactions_24h': data['interactions_24h'],
            'social_volume_24h': data['social_volume_24h']
        }

    return result


def calculate_all_indices(market_data: List[Dict]) -> Dict[str, Dict]:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã"""
    all_results = {}

    logger.info("–ù–∞—á–∞–ª–æ —Ä–∞—Å—á–µ—Ç–∞ —Å–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤")

    for index_code, index_info in INDICES.items():
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω–¥–µ–∫—Å–∞: {index_info['name']} ({len(index_info['tokens'])} —Ç–æ–∫–µ–Ω–æ–≤)")

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è —ç—Ç–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        filtered_tokens = filter_index_tokens(market_data, index_info['tokens'])

        if filtered_tokens:
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å
            index_result = calculate_single_index(filtered_tokens, index_code, index_info)

            if index_result:
                all_results[index_code] = index_result
                logger.info(f"–ò–Ω–¥–µ–∫—Å {index_code} —Ä–∞—Å—Å—á–∏—Ç–∞–Ω: {index_result['index_value']}/10 ({len(filtered_tokens)}/{len(index_info['tokens'])} —Ç–æ–∫–µ–Ω–æ–≤)")
        else:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {index_code}")

    return all_results


def save_to_supabase(result: Dict) -> bool:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–µ–∫—Å–∞ –≤ Supabase"""
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
        data = {
            'index_code': result['index_code'],
            'index_name': result['index_name'],
            'index_value': float(result['index_value']),
            'tokens_count': result['tokens_count'],
            'raw_metrics': result['raw_metrics'],
            'normalized_metrics': result['normalized_metrics'],
            'weights': result['weights'],
            'tokens': result['tokens'],
            'timestamp': result['timestamp']
        }

        # –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü—É
        response = supabase.table('index_metrics').insert(data).execute()

        if response.data:
            logger.info(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è {result['index_code']} —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Supabase")
            return True
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ {result['index_code']} –≤ Supabase")
            return False

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ Supabase: {e}")
        return False


def save_all_indices_to_supabase(results: Dict[str, Dict]) -> bool:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ Supabase"""
    success_count = 0

    logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Supabase...")

    for index_code, result in results.items():
        if save_to_supabase(result):
            success_count += 1

    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–Ω–¥–µ–∫—Å–æ–≤: {success_count}/{len(results)}")
    return success_count == len(results)


def clean_old_records(days_to_keep: int = 7):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    try:
        cutoff_date = (datetime.now() - timedelta(days=days_to_keep)).isoformat()

        response = supabase.table('index_metrics')\
            .delete()\
            .lt('timestamp', cutoff_date)\
            .execute()

        deleted_count = len(response.data) if response.data else 0
        logger.info(f"–£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π: {deleted_count}")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π: {e}")


def test_supabase_connection() -> bool:
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase"""
    try:
        response = supabase.table('index_metrics').select('*').limit(1).execute()
        logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Supabase: {e}")
        return False


def run_multi_index_calculation():
    """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ä–∞—Å—á–µ—Ç–∞ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ Supabase"""
    logger.info(f"–ó–∞–ø—É—Å–∫ –º—É–ª—å—Ç–∏–∏–Ω–¥–µ–∫—Å–Ω–æ–≥–æ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase
    if not test_supabase_connection():
        logger.error("–ü—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ Supabase")
        return None

    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å API
    market_data = fetch_market_data()
    if not market_data:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å API")
        return None

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Å–µ –∏–Ω–¥–µ–∫—Å—ã
    all_results = calculate_all_indices(market_data)

    if not all_results:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞")
        return None

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Supabase
    if save_all_indices_to_supabase(all_results):
        logger.info("–í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    else:
        logger.warning("–ù–µ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
    clean_old_records(days_to_keep=7)

    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    for index_code, result in all_results.items():
        logger.info(f"{result['index_name']}: {result['index_value']}/10 ({result['tokens_count']} —Ç–æ–∫–µ–Ω–æ–≤)")

    logger.info(f"–†–∞—Å—á–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–Ω–¥–µ–∫—Å–æ–≤: {len(all_results)}")
    return all_results


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã —Å–±–æ—Ä–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        required_vars = [
            'LUNARCRUSH_API_KEY', 'SUPABASE_URL', 'SUPABASE_KEY',
            'POSTGRES_USER', 'POSTGRES_PASSWORD', 'POSTGRES_HOST', 'POSTGRES_DATABASE'
        ]
        
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        if missing_vars:
            raise EnvironmentError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {', '.join(missing_vars)}")

        # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –ª–æ–≥–∏–∫–∏
        results = run_multi_index_calculation()
        
        if results:
            logger.info("‚úÖ –ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return 0
        else:
            logger.error("‚ùå –ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–∞–º–∏")
            return 1
            
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
